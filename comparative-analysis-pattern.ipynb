{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U 'tensorflow[and-cuda]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, VGG19, InceptionV3, Xception, ResNet50, DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\junu\\.cache\\kagglehub\\datasets\\vipoooool\\new-plant-diseases-dataset\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"vipoooool/new-plant-diseases-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\n",
    "# valid = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'\n",
    "train = \"C:\\\\Users\\\\junu\\\\.cache\\\\kagglehub\\\\datasets\\\\vipoooool\\\\new-plant-diseases-dataset\\\\versions\\\\2\\\\New Plant Diseases Dataset(Augmented)\\\\New Plant Diseases Dataset(Augmented)\\\\train\"\n",
    "valid = \"C:\\\\Users\\\\junu\\\\.cache\\\\kagglehub\\\\datasets\\\\vipoooool\\\\new-plant-diseases-dataset\\\\versions\\\\2\\\\New Plant Diseases Dataset(Augmented)\\\\New Plant Diseases Dataset(Augmented)\\\\valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70295 files belonging to 38 classes.\n",
      "Found 17572 files belonging to 38 classes.\n",
      "Number of classes: 38\n",
      "Image size: (128, 128)\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED: Larger image size for better feature extraction\n",
    "image_size = (128, 128)\n",
    "batch_size = 64\n",
    "\n",
    "# Training dataset - use the entire train folder\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train,\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation dataset - use the entire valid folder (NO validation_split!)\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid,\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=False  # Don't shuffle validation data\n",
    ")\n",
    "\n",
    "# SAVE class_names BEFORE applying transformations\n",
    "class_names = train_dataset.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Image size: {image_size}\")\n",
    "\n",
    "# IMPROVED: Enhanced data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomFlip(\"vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Apply augmentation and prefetching to training data\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Apply normalization and prefetching to validation data\n",
    "normalization = tf.keras.layers.Rescaling(1./255)\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x, y: (normalization(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Enhanced model architecture with deeper layers and dropout\n",
    "def create_model(base_model, model_name):\n",
    "    \"\"\"\n",
    "    Create an improved model with:\n",
    "    - Deeper dense layers\n",
    "    - Dropout for regularization\n",
    "    - BatchNormalization for stability\n",
    "    \"\"\"\n",
    "    # Initially freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # IMPROVED: Deeper architecture with more neurons\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with float32 for numerical stability\n",
    "    predictions = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # IMPROVED: Higher initial learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),  # Increased from 0.0001\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        jit_compile=True\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to fine-tune model\n",
    "def fine_tune_model(model, base_model, num_layers_to_unfreeze=20):\n",
    "    \"\"\"\n",
    "    Fine-tune the model by unfreezing top layers of base model\n",
    "    \"\"\"\n",
    "    # Unfreeze the base model\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze all layers except the last num_layers_to_unfreeze\n",
    "    for layer in base_model.layers[:-num_layers_to_unfreeze]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Recompile with lower learning rate for fine-tuning\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),  # Lower LR for fine-tuning\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 model created successfully\n",
      "VGG19 model created successfully\n",
      "InceptionV3 model created successfully\n",
      "Xception model created successfully\n",
      "ResNet50 model created successfully\n",
      "DenseNet121 model created successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize models with proper input shapes\n",
    "# Note: InceptionV3 and Xception require minimum 75x75, but 299x299 is optimal\n",
    "# For consistency, we use 224x224 for all models\n",
    "\n",
    "base_models = {\n",
    "    'VGG16': VGG16(weights='imagenet', include_top=False, input_shape=image_size + (3,)),\n",
    "    'VGG19': VGG19(weights='imagenet', include_top=False, input_shape=image_size + (3,)),\n",
    "    'InceptionV3': InceptionV3(weights='imagenet', include_top=False, input_shape=image_size + (3,)),\n",
    "    'Xception': Xception(weights='imagenet', include_top=False, input_shape=image_size + (3,)),\n",
    "    'ResNet50': ResNet50(weights='imagenet', include_top=False, input_shape=image_size + (3,)),\n",
    "    'DenseNet121': DenseNet121(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, base_model in base_models.items():\n",
    "    models[name] = create_model(base_model, name)\n",
    "    print(f\"{name} model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Define callbacks for better training\n",
    "def get_callbacks(model_name):\n",
    "    \"\"\"\n",
    "    Create callbacks for training:\n",
    "    - EarlyStopping: Stop when validation accuracy plateaus\n",
    "    - ReduceLROnPlateau: Reduce learning rate when stuck\n",
    "    - ModelCheckpoint: Save best model weights\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        # Stop training if val_accuracy doesn't improve for 5 epochs\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate if val_loss doesn't improve for 3 epochs\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Save best model weights\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'{model_name}_best.weights.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING STRATEGY:\n",
      "Stage 1: Initial training for 10 epochs (base model frozen)\n",
      "Stage 2: Fine-tuning for 10 epochs (top layers unfrozen)\n",
      "Total epochs: 20\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING VGG16\n",
      "================================================================================\n",
      "\n",
      "[STAGE 1] Initial training with frozen base model...\n",
      "Epoch 1/10\n",
      "\u001b[1m   6/1099\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31:13\u001b[0m 5s/step - accuracy: 0.0232 - loss: 4.5703"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# STAGE 1: Initial Training\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[STAGE 1] Initial training with frozen base model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m history_stage1 = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_stage1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# STAGE 2: Fine-tuning\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[STAGE 2] Fine-tuning with unfrozen top layers...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\junu\\Documents\\Python\\Model Comparative Analysis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# IMPROVED: Two-stage training process\n",
    "history = {}\n",
    "initial_epochs = 10  # Initial training with frozen base\n",
    "fine_tune_epochs = 10  # Fine-tuning with unfrozen layers\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING STRATEGY:\")\n",
    "print(f\"Stage 1: Initial training for {initial_epochs} epochs (base model frozen)\")\n",
    "print(f\"Stage 2: Fine-tuning for {fine_tune_epochs} epochs (top layers unfrozen)\")\n",
    "print(f\"Total epochs: {total_epochs}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # STAGE 1: Initial Training\n",
    "    print(f\"\\n[STAGE 1] Initial training with frozen base model...\")\n",
    "    history_stage1 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=initial_epochs,\n",
    "        callbacks=get_callbacks(f\"{model_name}_stage1\")\n",
    "    )\n",
    "    \n",
    "    # STAGE 2: Fine-tuning\n",
    "    print(f\"\\n[STAGE 2] Fine-tuning with unfrozen top layers...\")\n",
    "    model = fine_tune_model(model, base_models[model_name], num_layers_to_unfreeze=20)\n",
    "    \n",
    "    history_stage2 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=fine_tune_epochs,\n",
    "        callbacks=get_callbacks(f\"{model_name}_stage2\")\n",
    "    )\n",
    "    \n",
    "    # Combine histories\n",
    "    history[model_name] = {\n",
    "        'stage1': history_stage1,\n",
    "        'stage2': history_stage2\n",
    "    }\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(f\"{model_name}_plant_disease_model_improved.h5\")\n",
    "    print(f\"\\n{model_name} training completed and model saved.\")\n",
    "    \n",
    "    # Get predictions\n",
    "    print(f\"\\nGenerating predictions for {model_name}...\")\n",
    "    y_pred = model.predict(val_dataset)\n",
    "    \n",
    "    # Extract true labels from the dataset\n",
    "    y_true = []\n",
    "    for image_batch, label_batch in val_dataset:\n",
    "        y_true.append(label_batch)\n",
    "    \n",
    "    # Concatenate all batches into a single array\n",
    "    y_true = tf.concat(y_true, axis=0).numpy()\n",
    "    \n",
    "    # Save predictions with the model name\n",
    "    np.save(f\"{model_name}_y_pred_improved.npy\", y_pred)\n",
    "    np.save(f\"{model_name}_y_true_improved.npy\", y_true)\n",
    "    print(f'{model_name} predictions saved')\n",
    "    \n",
    "    # Calculate and display final accuracy\n",
    "    y_pred_labels = y_pred.argmax(axis=1)\n",
    "    y_true_labels = y_true.argmax(axis=1)\n",
    "    final_accuracy = np.mean(y_pred_labels == y_true_labels)\n",
    "    print(f\"\\n{model_name} FINAL VALIDATION ACCURACY: {final_accuracy*100:.2f}%\")\n",
    "    \n",
    "    if final_accuracy >= 0.985:\n",
    "        print(f\"✓ {model_name} achieved >=98.5% accuracy!\")\n",
    "    else:\n",
    "        print(f\"✗ {model_name} did not reach 98.5% accuracy (got {final_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS TRAINING COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL TEST EVALUATION (Run this AFTER all training is complete)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST EVALUATION ON UNSEEN DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load test dataset\n",
    "test = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/test'\n",
    "\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test,\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=False  # Important: don't shuffle test data\n",
    ")\n",
    "\n",
    "# Apply only normalization (NO augmentation for test!)\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda x, y: (normalization(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Evaluate each model on test set\n",
    "test_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing {model_name} on unseen test data...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load the saved model\n",
    "    model = tf.keras.models.load_model(f\"{model_name}_plant_disease_model_improved.h5\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=1)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_test_pred = model.predict(test_dataset)\n",
    "    \n",
    "    # Extract true labels\n",
    "    y_test_true = []\n",
    "    for image_batch, label_batch in test_dataset:\n",
    "        y_test_true.append(label_batch)\n",
    "    y_test_true = tf.concat(y_test_true, axis=0).numpy()\n",
    "    \n",
    "    # Convert to class labels\n",
    "    y_test_pred_labels = y_test_pred.argmax(axis=1)\n",
    "    y_test_true_labels = y_test_true.argmax(axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    precision = precision_score(y_test_true_labels, y_test_pred_labels, average='weighted')\n",
    "    recall = recall_score(y_test_true_labels, y_test_pred_labels, average='weighted')\n",
    "    f1 = f1_score(y_test_true_labels, y_test_pred_labels, average='weighted')\n",
    "    \n",
    "    test_results.append({\n",
    "        'Model': model_name,\n",
    "        'Test Accuracy (%)': test_accuracy * 100,\n",
    "        'Precision (%)': precision * 100,\n",
    "        'Recall (%)': recall * 100,\n",
    "        'F1-Score (%)': f1 * 100,\n",
    "        'Test Loss': test_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{model_name} Test Results:\")\n",
    "    print(f\"  Accuracy:  {test_accuracy*100:.2f}%\")\n",
    "    print(f\"  Precision: {precision*100:.2f}%\")\n",
    "    print(f\"  Recall:    {recall*100:.2f}%\")\n",
    "    print(f\"  F1-Score:  {f1*100:.2f}%\")\n",
    "    print(f\"  Loss:      {test_loss:.4f}\")\n",
    "\n",
    "# Create comparison table\n",
    "test_comparison_df = pd.DataFrame(test_results)\n",
    "test_comparison_df = test_comparison_df.sort_values('Test Accuracy (%)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + test_comparison_df.to_string(index=False))\n",
    "\n",
    "# Save test results\n",
    "test_comparison_df.to_csv('test_results_comparison.csv', index=False)\n",
    "print(\"\\n✓ Test results saved to 'test_results_comparison.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Model Comparison Table\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model_names = [\"VGG16\", \"VGG19\", \"InceptionV3\", \"Xception\", \"ResNet50\", \"DenseNet121\"]\n",
    "results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name in model_names:\n",
    "    y_pred = np.load(f\"{name}_y_pred_improved.npy\")\n",
    "    y_true = np.load(f\"{name}_y_true_improved.npy\")\n",
    "    \n",
    "    # Convert predictions and true labels from one-hot to class indices\n",
    "    y_pred_labels = y_pred.argmax(axis=1)\n",
    "    y_true_labels = y_true.argmax(axis=1)\n",
    "    \n",
    "    accuracy = np.mean(y_pred_labels == y_true_labels)\n",
    "    \n",
    "    # Calculate top-5 accuracy\n",
    "    top5_pred = np.argsort(y_pred, axis=1)[:, -5:]\n",
    "    top5_accuracy = np.mean([y_true_labels[i] in top5_pred[i] for i in range(len(y_true_labels))])\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy (%)': accuracy * 100,\n",
    "        'Top-5 Accuracy (%)': top5_accuracy * 100,\n",
    "        'Meets Target (>98.5%)': '✓' if accuracy > 0.985 else '✗'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "comparison_df = comparison_df.sort_values('Accuracy (%)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Count how many models achieved >98.5%\n",
    "models_above_target = comparison_df[comparison_df['Meets Target (>98.5%)'] == '✓'].shape[0]\n",
    "print(f\"\\nModels achieving >98.5% accuracy: {models_above_target}/{len(model_names)}\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('model_comparison_improved.csv', index=False)\n",
    "print(\"\\nComparison table saved to 'model_comparison_improved.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification reports for each model\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name in model_names:\n",
    "    y_pred = np.load(f\"{name}_y_pred_improved.npy\")\n",
    "    y_true = np.load(f\"{name}_y_true_improved.npy\")\n",
    "    \n",
    "    # Convert both predictions and true labels from one-hot to class indices\n",
    "    y_pred_labels = y_pred.argmax(axis=1)\n",
    "    y_true_labels = y_true.argmax(axis=1)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{name} - Classification Report\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_true_labels, y_pred_labels, target_names=class_names)\n",
    "    print(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open(f\"{name}_classification_report_improved.txt\", 'w') as f:\n",
    "        f.write(f\"{name} - Classification Report\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"Report saved to '{name}_classification_report_improved.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history for best performing model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the best model based on accuracy\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"Visualizing training history for best model: {best_model_name}\")\n",
    "\n",
    "if best_model_name in history:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Combine stage 1 and stage 2 histories\n",
    "    stage1 = history[best_model_name]['stage1']\n",
    "    stage2 = history[best_model_name]['stage2']\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(stage1.history['accuracy'] + stage2.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0].plot(stage1.history['val_accuracy'] + stage2.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0].axvline(x=len(stage1.history['accuracy']), color='r', linestyle='--', label='Fine-tuning starts')\n",
    "    axes[0].axhline(y=0.985, color='g', linestyle='--', label='Target (98.5%)')\n",
    "    axes[0].set_title(f'{best_model_name} - Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(stage1.history['loss'] + stage2.history['loss'], label='Training Loss')\n",
    "    axes[1].plot(stage1.history['val_loss'] + stage2.history['val_loss'], label='Validation Loss')\n",
    "    axes[1].axvline(x=len(stage1.history['loss']), color='r', linestyle='--', label='Fine-tuning starts')\n",
    "    axes[1].set_title(f'{best_model_name} - Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{best_model_name}_training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training history plot saved to '{best_model_name}_training_history.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal models trained: {len(model_names)}\")\n",
    "print(f\"Models achieving >98.5% accuracy: {models_above_target}\")\n",
    "print(f\"\\nBest performing model: {comparison_df.iloc[0]['Model']}\")\n",
    "print(f\"Best accuracy: {comparison_df.iloc[0]['Accuracy (%)']:.2f}%\")\n",
    "print(\"\\nKey improvements implemented:\")\n",
    "print(\"  ✓ Increased image size to 224x224\")\n",
    "print(\"  ✓ Enhanced architecture with deeper layers and dropout\")\n",
    "print(\"  ✓ Two-stage training with fine-tuning\")\n",
    "print(\"  ✓ Learning rate scheduling\")\n",
    "print(\"  ✓ Early stopping and model checkpointing\")\n",
    "print(\"  ✓ Enhanced data augmentation\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true,
   "isInternetEnabled": true
  },
  "kernelspec": {
   "display_name": "pattern (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
